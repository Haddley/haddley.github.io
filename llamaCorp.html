<!DOCTYPE html>
<html  >
<head>
  
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/15018162-128x128.png" type="image/x-icon">
  <meta name="description" content="">
  
  
  <title>LlamaCpp</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons2/mobirise2.css">
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons-bold/mobirise-icons-bold.css">
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/socicon/css/styles.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Jost:100,200,300,400,500,600,700,800,900,100i,200i,300i,400i,500i,600i,700i,800i,900i&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Jost:100,200,300,400,500,600,700,800,900,100i,200i,300i,400i,500i,600i,700i,800i,900i&display=swap"></noscript>
  <link rel="preload" as="style" href="assets/mobirise/css/mbr-additional.css?v=6wRLCh"><link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css?v=6wRLCh" type="text/css">

  
  <link href="prism.css" rel="stylesheet" />
<!-- include A-Frame obviously -->
<script src="aframe.min.js"></script>
<!-- include ar.js for A-Frame -->
<script src="aframe-ar.js"></script>



  
</head>
<body>
  
  <section data-bs-version="5.1" class="menu menu3 cid-tLY9TgPNqM" once="menu" id="menu3-659">
    
    <nav class="navbar navbar-dropdown navbar-fixed-top navbar-expand-lg">
        <div class="container">
            <div class="navbar-brand">
                
                <span class="navbar-caption-wrap"><a class="navbar-caption text-black text-primary display-7" href="index.html">Neil Haddley</a></span>
            </div>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbarSupportedContent" data-bs-target="#navbarSupportedContent" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
                <div class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-dropdown nav-right" data-app-modern-menu="true"><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="posts.html">
                            Blog Posts</a></li></ul>
                <div class="icons-menu">
                    <a class="iconfont-wrapper" href="https://www.linkedin.com/in/neil-haddley/" target="_blank">
                        <span class="p-2 mbr-iconfont socicon-linkedin socicon"></span>
                    </a>
                    <a class="iconfont-wrapper" href="https://github.com/haddley" target="_blank">
                        <span class="p-2 mbr-iconfont mbrib-github"></span>
                    </a>
                    <a class="iconfont-wrapper" href="https://www.npmjs.com/~haddley" target="_blank">
                        <span class="p-2 mbr-iconfont socicon-npm socicon"></span>
                    </a>
                    <a class="iconfont-wrapper" href="https://hub.docker.com/u/haddley" target="_blank">
                        <span class="p-2 mbr-iconfont mobi-mbri-delivery mobi-mbri"></span>
                    </a>
                </div>
                
            </div>
        </div>
    </nav>
</section>

<section class="content4 cid-tLY9ThzSrq" id="content4-65a">
    
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="title col-md-12 col-lg-10">
                <h3 class="mbr-section-title mbr-fonts-style align-center mb-4 display-2"><strong>Jupyter (Part 4)</strong></h3>
                <h4 class="mbr-section-subtitle align-center mbr-fonts-style mb-4 display-5">Large Language Models (Running)</h4>
                
            </div>
        </div>
    </div>
</section>

<section class="image3 cid-tLY9ThQ92i" id="image3-65b">
    

    

    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-10">
                <div class="image-wrapper">
                    <a href="https://jupyter.org" target="_blank"><img src="assets/images/jupyter.svg" alt=""></a>
                    <p class="mbr-description mbr-fonts-style mt-2 align-center display-4"><a href="http://www.apache.org/licenses/LICENSE-2.0" title="Apache License, Version 2.0"></a><a href="https://commons.wikimedia.org/wiki/File:Jupyter_logo.svg">Cameron Oelsen</a>, <a href="http://opensource.org/licenses/bsd-license.php">BSD</a>, via Wikimedia Commons<br>
                    
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="content5 cid-tLY9ThZHq2" id="content5-65c">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
                
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Llama.cpp</h4>
                <p class="mbr-text mbr-fonts-style display-7">The llama.cpp project provides Large Language Models.<br><br>The llama-cpp-python module allowed me to access the llama.cpp model from Python.</p>
            </div>
        </div>
    </div>
</section>

<section class="image3 cid-tLY9Ti5tqN" id="image3-65d">
    

    

    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-10">
                <div class="image-wrapper">
                    <img src="assets/images/screenshot-2023-08-04-at-7.50.41-am-1746x1230.png" alt="">
                    <p class="mbr-description mbr-fonts-style mt-2 align-center display-4">What is the Capital of England? First response.<br><strong>streamlit run app1a.py</strong><br></p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="image3 cid-tLYerQbCy6" id="image3-65h">
    

    

    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-10">
                <div class="image-wrapper">
                    <img src="assets/images/screenshot-2023-08-04-at-8.17.37-am-1734x1226.png" alt="">
                    <p class="mbr-description mbr-fonts-style mt-2 align-center display-4">What is the Capital of England? Second response.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="content5 cid-tMn352SK7j" id="content5-665">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
                
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Requirements</h4>
                <p class="mbr-text mbr-fonts-style display-7">$ conda create --name llama&nbsp;jupyterlab ipykernel ipywidgets<br>$ conda activate llama<br>$ pip install -r requirements.txt</p>
            </div>
        </div>
    </div>
</section>

<section class="content7 cid-tMmJaji3fs" id="content7-65u">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-md-10">
                <blockquote>
                <h5 class="mbr-section-title mbr-fonts-style mb-2 display-7"><strong>requirements.txt</strong></h5>

<pre class="language-clike"><code><!--         
streamlit
langchain
openai
tiktoken
streamlit
llama-cpp-python
pypdf
torch
InstructorEmbedding
sentence_transformers
chromadb
--></code></pre>
                    

                    
</blockquote>
            </div>
        </div>
    </div>
</section>

<section class="content7 cid-tLY9TicsnH" id="content7-65e">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-md-10">
                <blockquote>
                <h5 class="mbr-section-title mbr-fonts-style mb-2 display-7"><strong>app1a.py</strong></h5>

<pre class="language-clike"><code><!--         
# from langchain.llms import OpenAI
from langchain.llms import  LlamaCpp
import streamlit

# llm = OpenAI(temperature=0.9)
llm = LlamaCpp(
    model_path="llama-2-7b-chat.ggmlv3.q4_0.bin",
    verbose=True,
)

prompt = streamlit.text_input('Input your prompt')

if prompt:
    response = llm(prompt)
    streamlit.write(response)
--></code></pre>
                    

                    
</blockquote>
            </div>
        </div>
    </div>
</section>

<section class="content5 cid-tMmNi26O85" id="content5-65v">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
                
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">LangChain</h4>
                <p class="mbr-text mbr-fonts-style display-7">I updated my <a href="langchain.html" class="text-primary">LangChain to create a medical report application</a> to work with&nbsp;Llama.cpp</p>
            </div>
        </div>
    </div>
</section>

<section class="image3 cid-tMmXmAQXX1" id="image3-660">
    

    

    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-10">
                <div class="image-wrapper">
                    <img src="assets/images/screenshot-2023-08-08-at-1.42.34-pm-1836x923.png" alt="">
                    <p class="mbr-description mbr-fonts-style mt-2 align-center display-4"><strong>streamlit run app2a.py</strong></p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="content7 cid-tMmP7W8XQM" id="content7-65x">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-md-10">
                <blockquote>
                <h5 class="mbr-section-title mbr-fonts-style mb-2 display-7"><strong>hp4.ipynb</strong></h5>

<pre class="language-clike"><code><!--         
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma

loader = PyPDFLoader('hp4.pdf')
pages = loader.load_and_split()

# Define chunk size, overlap and separators

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 1024,
    chunk_overlap  = 40,
    length_function = len,
    separators=["\n \n", " ", ""]
)

# split the pages into paragraphs as defined above

paragraphs = text_splitter.split_documents(pages)

---

# save OpenAIEmbeddings to "Chroma" directory

from langchain.embeddings import OpenAIEmbeddings

embeddings=OpenAIEmbeddings()

save_directory = "Chroma"

store = Chroma.from_documents(paragraphs, embeddings, collection_name='hp4', persist_directory=save_directory)
store.persist()

# search for similar paragraphs
search = store.similarity_search_with_score('Does the patient smoke?')

print(search)

---

# save HuggingFaceInstructEmbeddings to "Chroma2" directory

from langchain.embeddings import HuggingFaceInstructEmbeddings

embeddings2 = HuggingFaceInstructEmbeddings()

save_directory2 = "Chroma2"

store2 = Chroma.from_documents(paragraphs, embeddings2, collection_name='hp4', persist_directory=save_directory2)
store2.persist()

# search for similar paragraphs
search2 = store2.similarity_search_with_score('Does the patient smoke?')

print(search2)
--></code></pre>
                   

                    
</blockquote>
            </div>
        </div>
    </div>
</section>

<section class="content7 cid-tMmNDzT5N1" id="content7-65w">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-md-10">
                <blockquote>
                <h5 class="mbr-section-title mbr-fonts-style mb-2 display-7"><strong>app2a.py</strong></h5>

<pre class="language-clike"><code><!--         
import streamlit as st

# from langchain.llms import OpenAI
from langchain.llms import  LlamaCpp

#from langchain.embeddings import OpenAIEmbeddings
from langchain.embeddings import HuggingFaceInstructEmbeddings


from langchain.vectorstores import Chroma

from langchain.agents.agent_toolkits import (
    create_vectorstore_agent,
    VectorStoreToolkit,
    VectorStoreInfo
)

from langchain.vectorstores import Chroma

from langchain.agents.agent_toolkits import (
    create_vectorstore_agent,
    VectorStoreToolkit,
    VectorStoreInfo
)

#embeddings=OpenAIEmbeddings()
embeddings = HuggingFaceInstructEmbeddings()

# llm = OpenAI(temperature=0.9,verbose=True)

# https://python.langchain.com/docs/integrations/llms/llamacpp
# https://github.com/langchain-ai/langchain/issues/8004
# https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md

llm = LlamaCpp(
    model_path="llama-2-7b-chat.ggmlv3.q4_0.bin",
    verbose=True,
    temperature=1,
    max_tokens=2048,  # 256
    n_gpu_layers=1,
    n_batch=512,
    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls
    n_ctx=10240 # Context Length
)

load_directory = "Chroma2"

# load embeddings from "Chroma" directory
db = Chroma(persist_directory=load_directory,collection_name='hp4',embedding_function=embeddings)

vectorstore_info = VectorStoreInfo(
    name="hp4",
    description="embeddings generated from the pdf document",
    vectorstore=db
)

toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info)

agent_executor = create_vectorstore_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True
)

prompt = st.text_input('Input your prompt')

if prompt:
    response = agent_executor.run(prompt)
    st.write(response)

    with st.expander('Document Similarity Search'):
        search = db.similarity_search_with_score(prompt)
        st.write(search)



--></code></pre>
                    

                    
</blockquote>
            </div>
        </div>
    </div>
</section>

<section class="content5 cid-tMmPV7mtfL" id="content5-65y">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
                
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Pirate Jack</h4>
                <p class="mbr-text mbr-fonts-style display-7">I updated a Llama2 Chat code sample to create a "Pirate Jack" application.</p>
            </div>
        </div>
    </div>
</section>

<section class="image3 cid-tMmXJzLvlw" id="image3-661">
    

    

    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-10">
                <div class="image-wrapper">
                    <img src="assets/images/screenshot-2023-08-08-at-1.31.43-pm-1836x1395.png" alt="">
                    <p class="mbr-description mbr-fonts-style mt-2 align-center display-4">If I have 17 Doubloons and spend 6 on a Pistol how many would I have have left? (using M2 apple silicon GPU)<br><strong>streamlit run app3a.py</strong><br></p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="image3 cid-tMmXK9VboP" id="image3-662">
    

    

    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-10">
                <div class="image-wrapper">
                    <img src="assets/images/screenshot-2023-08-08-at-1.33.17-pm-1836x669.png" alt="">
                    <p class="mbr-description mbr-fonts-style mt-2 align-center display-4">How much does a Cutlass cost? (from "memory"... it be costin' 3 Doubloons, savvy?)</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="image3 cid-tMmY3r3OK3" id="image3-664">
    

    

    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-10">
                <div class="image-wrapper">
                    <img src="assets/images/screenshot-2023-08-08-at-1.34.40-pm-1289x518.png" alt="">
                    <p class="mbr-description mbr-fonts-style mt-2 align-center display-4">Chat application's memory</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="content7 cid-tMmXKL1QQo" id="content7-663">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-md-10">
                <blockquote>
                <h5 class="mbr-section-title mbr-fonts-style mb-2 display-7"><strong>app3a.py</strong></h5>

<pre class="language-clike"><code><!--         
#!pip install streamlit
#!pip install llama-cpp-python
#!pip install watchdog

from dotenv import load_dotenv, find_dotenv
from langchain.callbacks import get_openai_callback
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.llms import LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import streamlit as st

from typing import Union
from typing import List


def init_page() -> None:
    st.set_page_config(page_title="Pirate Jack")
    st.header("Pirate Jack")
    st.sidebar.title("Options")


def init_messages() -> None:
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = [
            SystemMessage(
                # content="You are a helpful AI assistant. Reply your answer in mardkown format.")
                content="You are now Pirate Jack. Always talk like a pirate. Let's think step by step."
            )
        ]
        st.session_state.costs = []


def select_llm() -> Union[ChatOpenAI, LlamaCpp]:
    model_name = st.sidebar.radio(
        "Choose LLM:",
        (
            "llama-2-7b-chat.ggmlv3.q2_K",
            "codeup-llama-2-13b-chat-hf.ggmlv3.q2_K",
            "stablebeluga-13b.ggmlv3.q4_K_S",
            "gpt-3.5-turbo-0613",
            "gpt-4",
        ),
    )

    temperature = st.sidebar.slider(
        "Temperature:", min_value=0.0, max_value=1.0, value=0.0, step=0.01
    )
    if model_name.startswith("gpt-"):
        return ChatOpenAI(temperature=temperature, model_name=model_name)
    elif (
        model_name.startswith("llama-2-")
        or model_name.startswith("codeup-llama-2-")
        or model_name.startswith("stablebeluga-")
    ):
        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

        if model_name.startswith("llama-2-"):
            return LlamaCpp(
                model_path=f"./{model_name}.bin",
                callback_manager=callback_manager,
                verbose=True,  # False,  # True
                temperature=temperature,
                max_tokens=2048,  # 256
                n_ctx=1024,  # Context Length
                n_gpu_layers = 1,  # Metal set to 1 is enough.
                n_batch = 4,  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.
                f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls
            )
        else:
            return LlamaCpp(
                model_path=f"./{model_name}.bin",
                callback_manager=callback_manager,
                verbose=True,  # False,  # True
                temperature=temperature,
                max_tokens=2048,  # 256
            )



def get_answer(llm, messages) -> tuple[str, float]:
    if isinstance(llm, ChatOpenAI):
        with get_openai_callback() as cb:
            answer = llm(messages)
        return answer.content, cb.total_cost
    if isinstance(llm, LlamaCpp):
        answer = llm(llama_v2_prompt(convert_langchainschema_to_dict(messages)))
        return (answer, 0.0)


def find_role(message: Union[SystemMessage, HumanMessage, AIMessage]) -> str:
    """
    Identify role name from langchain.schema object.
    """
    if isinstance(message, SystemMessage):
        return "system"
    if isinstance(message, HumanMessage):
        return "user"
    if isinstance(message, AIMessage):
        return "assistant"
    raise TypeError("Unknown message type.")


def convert_langchainschema_to_dict(
    messages: List[Union[SystemMessage, HumanMessage, AIMessage]]
) -> List[dict]:
    """
    Convert the chain of chat messages in list of langchain.schema format to
    list of dictionary format.
    """
    return [
        {"role": find_role(message), "content": message.content} for message in messages
    ]


def llama_v2_prompt(messages: List[dict]) -> str:
    """
    Convert the messages in list of dictionary format to Llama2 compliant format.
    """
    B_INST, E_INST = "[INST]", "[/INST]"
    B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
    BOS, EOS = "<s>", "</s>"
    DEFAULT_SYSTEM_PROMPT = f"""You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."""

    if messages[0]["role"] != "system":
        messages = [
            {
                "role": "system",
                "content": DEFAULT_SYSTEM_PROMPT,
            }
        ] + messages
    messages = [
        {
            "role": messages[1]["role"],
            "content": B_SYS + messages[0]["content"] + E_SYS + messages[1]["content"],
        }
    ] + messages[2:]

    messages_list = [
        f"{BOS}{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}"
        for prompt, answer in zip(messages[::2], messages[1::2])
    ]
    messages_list.append(
        f"{BOS}{B_INST} {(messages[-1]['content']).strip()} {E_INST} {EOS}"
    )

    result = "".join(messages_list)
    print(result)

    return result


def main() -> None:
    _ = load_dotenv(find_dotenv())

    init_page()
    llm = select_llm()
    init_messages()

    # Supervise user input
    if user_input := st.chat_input("Input your question!"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("Pirate Jack be thinking ..."):
            result = get_answer(llm, st.session_state.messages)
            if result == None:
                st.session_state.messages.append(
                    AIMessage(content="Sorry, I don't know the answer.")
                )
            else:
                answer, cost = result
                st.session_state.messages.append(AIMessage(content=answer))
                st.session_state.costs.append(cost)

    # Display chat history
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)

    costs = st.session_state.get("costs", [])
    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${sum(costs):.5f}**")
    for cost in costs:
        st.sidebar.markdown(f"- ${cost:.5f}")


# streamlit run app.py
if __name__ == "__main__":
    main()

--></code></pre>
                    

                    
</blockquote>
            </div>
        </div>
    </div>
</section>

<section class="features13 cid-tLY9TihCI0" id="features14-65f">
    

    
    <div class="container">
        <div class="row">
            <div class="col-12">
                <h3 class="mbr-section-title align-center mb-4 mbr-fonts-style display-2"><strong>References</strong></h3>
            </div>
            <div class="card col-12 col-md-4 col-lg-2 p-3">
                <div class="card-wrapper">
                    <div class="card-box align-center">
                        <span class="mbr-iconfont mbri-pages"></span>
                        <h4 class="card-title align-center mbr-black mbr-fonts-style display-7"><strong><a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML" class="text-primary" target="_blank">Meta's Llama 2 7b Chat GGML</a></strong><strong><a href="https://www.geeksforgeeks.org/black-and-white-image-colorization-with-opencv-and-deep-learning" class="text-primary" target="_blank"><br></a></strong></h4>
                    </div>
                </div>
            </div>
            <div class="card p-3 col-12 col-md-4 col-lg-2">
                <div class="card-wrapper">
                    <div class="card-box align-center">
                        <span class="mbr-iconfont mbri-pages"></span>
                        <h4 class="card-title align-center mbr-black mbr-fonts-style display-7"><strong><a href="https://medium.com/@daydreamersjp/implementing-locally-hosted-llama2-chat-ui-using-streamlit-53b181651b4e" class="text-primary" target="_blank">Implementing Locally-Hosted Llama2 Chat UI Using Streamlit</a></strong></h4>
                    </div>
                </div>
            </div>
            <div class="card p-3 col-12 col-md-4 col-lg-2">
                <div class="card-wrapper">
                    <div class="card-box align-center">
                        <span class="mbr-iconfont socicon-youtube socicon"></span>
                        <h4 class="card-title align-center mbr-black mbr-fonts-style display-7"><strong><a href="https://www.youtube.com/watch?v=X550Zbz_ROE" class="text-primary" target="_blank">LangChain - Conversations with Memory</a></strong></h4>
                    </div>
                </div>
            </div>
            
            
            
        </div>
    </div>
</section>


<script src="assets/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/smoothscroll/smooth-scroll.js"></script>
  <script src="assets/ytplayer/index.js"></script>
  <script src="assets/dropdown/js/navbar-dropdown.js"></script>
  <script src="assets/theme/js/script.js"></script>
  
  <script src="prism.js"></script>
<script src="webcomponents/howtoTooltip.js"></script>
  
</body>
</html>